{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29cb74e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import all necessary tools\n",
    "\n",
    "from IPython.lib.deepreload import reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import measure\n",
    "import pandas as pd\n",
    "\n",
    "from glob import glob\n",
    "from tifffile import imread,imsave\n",
    "\n",
    "from stardist.models import StarDist3D\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "from utils_3Dsegmentation import (  \n",
    "    patch_segmentation, norm_X,\n",
    "    Hough_circle_3D, filter_regions_around_specific_voxel,\n",
    "    sort_out_interfacing_regions,\n",
    "    show_4d_with_contours, project_colours, project_inside_edge\n",
    ")\n",
    "\n",
    "# from src.utils_3Dsegmentation import (  # noqa: E402\n",
    "#     patch_segmentation,\n",
    "#     norm_X\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7189a0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory and results directories\n",
    "\n",
    "folder = 'Embryo1/'\n",
    "intensity_folder = 'intensity'\n",
    "segmentation_results = 'STAR-3D_results'\n",
    "filtered_results = 'STAR-3D_filtered'\n",
    "\n",
    "if not os.path.isdir(folder + segmentation_results):\n",
    "    os.makedirs(folder + segmentation_results)\n",
    "if not os.path.isdir(folder + filtered_results):\n",
    "    os.makedirs(folder + filtered_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700e1e2",
   "metadata": {},
   "source": [
    "## Automatic nuclei segmentation using STAR-3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7550f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "    \n",
    "resolution = np.array([2,0.174,0.174]) #in um\n",
    "# We have found the network to perform optimally when the objects to be detected are about 5 voxels in the lateral direction\n",
    "# and 60 voxels axially\n",
    "expected_object_diameter = 30 #um\n",
    "optimal_resolution = expected_object_diameter/np.array([5.0,60.0,60.0])\n",
    "downsampling = resolution/optimal_resolution\n",
    "gamma_factor = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37da8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load intensity files and model\n",
    "\n",
    "# Load intensity image files\n",
    "input_files = sorted(glob(folder + intensity_folder + '/*.tif'))[:]\n",
    "\n",
    "# Load model\n",
    "model = StarDist3D(None, name='star-3d', basedir= './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5caf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply STAR-3D to the images\n",
    "# This might take a very long time. There's a .py version of the segmentation part of the example script in src/ \n",
    "# for use on e.g. a cluster.\n",
    "\n",
    "for file in input_files:\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # Name the output file (this is normally part of the intensity image file name)\n",
    "    output_file_name = os.path.splitext(os.path.basename(file))[0][0:-6] + '_label.tif' \n",
    "\n",
    "    # Load image    \n",
    "    intensity = imread(file,is_ome=False)\n",
    "    \n",
    "    # Crop intensity image if possible. Applying STAR-3D can take a very long time so it is better to avoid \n",
    "    # segmenting empty parts of the image wherever possible. \n",
    "    # We have an embryo segmentation method currently in development which could automate this step\n",
    "    crop = np.array([[0,intensity.shape[0]],[0,intensity.shape[1]],[0,intensity.shape[2]]])\n",
    "    intensity_cropped = intensity[crop[0,0]:crop[0,1],crop[1,0]:crop[1,1],crop[2,0]:crop[2,1]]\n",
    "    \n",
    "    # Normalise, downsample, and gamma correct image\n",
    "    X_norm = norm_X(intensity_cropped,downsampling,gamma_factor)\n",
    "\n",
    "    # Apply STAR-3D in patches to avoid running into Tensorflow's tensor size limit\n",
    "    # Here we calculated the number of patches from the array size, but this method is not very reliable\n",
    "    # Increase patches if tensorflow returns an error\n",
    "    patches = 2**(X_norm.size>np.array([6e7,5e7,4e7])).astype(int) + 2**(X_norm.size>np.array([29e7,24e7,19e7])).astype(int) - 1\n",
    "    print(patches)\n",
    "    # Set margins where patches overlap to be bigger than the nuclei\n",
    "    margins = 40/(resolution/downsampling) # we expect the nuclei to be no more than 40 um in diameter\n",
    "    \n",
    "    # Apply STAR-3D and resample label map to the original resolution\n",
    "    Y_pred = scipy.ndimage.zoom(patch_segmentation(X_norm,model,patches,margins),\n",
    "                                np.asarray(intensity_cropped.shape)/np.asarray(X_norm.shape),order = 0)\n",
    "\n",
    "    # Separate different regions with the same label \n",
    "    # (although it shouldn't happen that two distinct regions have the same label, it's better to make sure)\n",
    "    Y_pred = measure.label(Y_pred.astype(int), connectivity=2)\n",
    "    \n",
    "    # Pad Y_pred to original size (especially if you're planning to use our tracking methods as well)\n",
    "    Y_pred_full = np.zeros(intensity.shape)\n",
    "    Y_pred_full[crop[0,0]:crop[0,1],crop[1,0]:crop[1,1],crop[2,0]:crop[2,1]] = Y_pred\n",
    "    \n",
    "    # Save unfiltered result\n",
    "    imsave(folder + segmentation_results + '/' + output_file_name, Y_pred_full.astype('uint16'), imagej=True, \n",
    "           resolution=1/resolution[1:3],\n",
    "           metadata={\n",
    "               'spacing': resolution[0],\n",
    "               'unit': 'um',\n",
    "               'axes': 'ZYX'\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72418ea6",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image-wide, property-based filtering\n",
    "\n",
    "source_folder = segmentation_results\n",
    "output_folder = filtered_results\n",
    "\n",
    "label_files = sorted(glob(folder + source_folder + '/*.tif'))[:]\n",
    "\n",
    "for file in label_files:\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # Name the output file (this is normally part of the original image file name)\n",
    "    output_file_name = os.path.splitext(os.path.basename(file))[0][0:-4] + '_filtered.tif'\n",
    "    \n",
    "    # Load label image\n",
    "    label_image = imread(file,is_ome=False)\n",
    "    \n",
    "    # Calculate the properties that you wish to filter for\n",
    "    props = pd.DataFrame(measure.regionprops_table(label_image,properties=[\"label\", \"area\", \"euler_number\", \"moments\"]))\n",
    "    \n",
    "    # Filter image\n",
    "    label_image[np.isin(label_image,props['label'][props['area']<100])] = 0\n",
    "    label_image[np.isin(label_image,props['label'][props['euler_number']<-1])] = 0\n",
    "    \n",
    "    # Save filtered image\n",
    "    imsave(folder + output_folder + '/' + output_file_name, label_image.astype('uint16'), imagej=True, \n",
    "           resolution=1/resolution[1:3],\n",
    "           metadata={\n",
    "               'spacing': resolution[0],\n",
    "               'unit': 'um',\n",
    "               'axes': 'ZYX'\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fbde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dust around the embryo\n",
    "# There's often a lot of dust around the embryo which get segmented by STAR-3D\n",
    "# We have prepared a few functions that might be useful for removing these\n",
    "\n",
    "###########################################################################################################\n",
    "###### We have an embryo segmentation method currently in development which could automate this step ######\n",
    "###### Also, it might be easier to remove the entire tracks of these unwanted structures ##################\n",
    "###### (see github.com/akarsa/optimal3dtracks) ############################################################\n",
    "###########################################################################################################\n",
    "\n",
    "source_folder = filtered_results\n",
    "output_folder = filtered_results\n",
    "\n",
    "label_files = sorted(glob(folder + source_folder + '/*.tif'))[:]\n",
    "intensity_files = sorted(glob(folder + intensity_folder + '/*.tif'))[:]\n",
    "\n",
    "assert all(Path(input_file_1).name[0:7]==Path(input_file_2).name[0:7] \n",
    "           for input_file_1, input_file_2 in zip(intensity_files,label_files))\n",
    "\n",
    "for file, file_int in zip(label_files,intensity_files):\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # Name the output file (this is normally part of the original image file name)\n",
    "    output_file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "    # Load label and intensity images\n",
    "    label_image = imread(file,is_ome=False)\n",
    "    intensity = imread(file_int,is_ome=False)\n",
    "    X_norm = norm_X(intensity,1,1)\n",
    "    \n",
    "    # Masking the embryo by fitting a sphere around it using 3D Hough transforms \n",
    "    # This actually works pretty well if the embryo is spherical. Unfortunately, this is not always the case.\n",
    "    hough_radii = np.linspace(25,75,50) # potential embryo radii\n",
    "    hough_resolution = np.array([2,0.174,0.174]) # resolution of the image\n",
    "    embryo_boundary = Hough_circle_3D(X_norm, hough_radii, hough_resolution)\n",
    "    label_image *= embryo_boundary\n",
    "    \n",
    "    # Supervised filtering of dust around a specific area of the image\n",
    "    resolution = np.array([2,0.174,0.174])\n",
    "    voxel = np.array([0,0,0]) # this will filter around the corner of the image\n",
    "    radius = 20 # in a 20 um radius\n",
    "    label_image = filter_regions_around_specific_voxel(label_image, X_norm, resolution, voxel, radius)\n",
    "    \n",
    "    # Save filtered image\n",
    "    imsave(folder + output_folder + '/' + output_file_name, label_image.astype('uint16'), imagej=True, \n",
    "           resolution=1/resolution[1:3],\n",
    "           metadata={\n",
    "               'spacing': resolution[0],\n",
    "               'unit': 'um',\n",
    "               'axes': 'ZYX'\n",
    "           })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6b41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised filtering of regions touching across a large surface\n",
    "# Sometimes STAR-3D creates subsegments within a large nuclei which are interfacing across a large surface\n",
    "# Here's a function that might be helpful in sorting out these instances.\n",
    "# It might be easier to do this on the tracks (see github.com/akarsa/optimal3dtracks)\n",
    "\n",
    "source_folder = filtered_results\n",
    "output_folder = filtered_results\n",
    "\n",
    "label_files = sorted(glob(folder + source_folder + '/*.tif'))[:]\n",
    "intensity_files = sorted(glob(folder + intensity_folder + '/*.tif'))[:]\n",
    "\n",
    "assert all(Path(input_file_1).name[0:7]==Path(input_file_2).name[0:7] \n",
    "           for input_file_1, input_file_2 in zip(intensity_files,label_files))\n",
    "\n",
    "for file, file_int in zip(label_files,intensity_files):\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # Name the output file (this is normally part of the original image file name)\n",
    "    output_file_name = os.path.splitext(os.path.basename(file))[0]\n",
    "    \n",
    "    # Load label and intensity images\n",
    "    label_image = imread(file,is_ome=False)\n",
    "    intensity = imread(file_int,is_ome=False)\n",
    "    X_norm = norm_X(intensity,1,1)\n",
    "    \n",
    "    # Supervised filtering of regions touching across a large surface\n",
    "    minimum_percentage_of_connection = 0.05 # below 5% of interfacing area, we assume that they're different nuclei\n",
    "                                            # supervised filtering is only performed above this threshold\n",
    "    label_image = sort_out_interfacing_regions(label_image, X_norm, minimum_percentage_of_connection)\n",
    "    \n",
    "    # Save filtered image\n",
    "    imsave(folder + output_folder + '/' + output_file_name, label_image.astype('uint16'), imagej=True, \n",
    "           resolution=1/resolution[1:3],\n",
    "           metadata={\n",
    "               'spacing': resolution[0],\n",
    "               'unit': 'um',\n",
    "               'axes': 'ZYX'\n",
    "           })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf4d4b0",
   "metadata": {},
   "source": [
    "## Display features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81da195e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A python widget for inspecting the segmentation as overlay on the intensity image\n",
    "\n",
    "label_files = sorted(glob(folder + filtered_results + '/*.tif'))[:]\n",
    "intensity_files = sorted(glob(folder + intensity_folder + '/*.tif'))[:]\n",
    "\n",
    "assert all(Path(input_file_1).name[0:7]==Path(input_file_2).name[0:7] \n",
    "           for input_file_1, input_file_2 in zip(intensity_files,label_files))\n",
    "\n",
    "labels = []\n",
    "intensities = []\n",
    "\n",
    "for file, file_int in zip(label_files,intensity_files):\n",
    "    \n",
    "    print(file)\n",
    "    \n",
    "    # Load label and intensity images\n",
    "    label_image = imread(file,is_ome=False)\n",
    "    intensity = imread(file_int,is_ome=False)\n",
    "    X_norm = norm_X(intensity,np.array([1,1,1]),1)\n",
    "    \n",
    "    labels.append(np.expand_dims(label_image,axis=0))\n",
    "    intensities.append(np.expand_dims(X_norm,axis=0))\n",
    "\n",
    "# Show 4D images with countours\n",
    "show_4d_with_contours(np.concatenate(intensities),np.concatenate(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9b7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a top-down view of the segmentations\n",
    "\n",
    "label_image = imread(sorted(glob(folder + filtered_results + '/*.tif'))[0],is_ome=False)\n",
    "\n",
    "plt.imshow(project_colours(label_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f400ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour segmentations according to their position in the morula (from edge (blue) to middle (red))\n",
    "\n",
    "label_image = imread(sorted(glob(folder + filtered_results + '/*.tif'))[0],is_ome=False)\n",
    "\n",
    "anisotropy = int(np.round(2/0.174)) # anisotropy of the voxel size\n",
    "distance_map, distance_map_masked = distance_from_edge_colouring(label_image, anisotropy)\n",
    "\n",
    "plt.imshow(project_inside_edge(distance_map_masked))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
